{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating Vocabulary**\n",
    "\n",
    "We are using the COCA samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 462883), ('to', 238874), ('and', 232584), ('of', 218668), ('a', 206816), ('in', 154602), ('i', 139623), ('that', 124059), ('you', 109927), ('p', 108290), ('s', 107201), ('it', 104072), ('is', 94201), ('for', 79002), ('on', 65490), ('was', 64461), ('with', 59800), ('he', 57779), ('this', 51981), ('t', 51527), ('as', 51304), ('n', 51142), ('we', 47814), ('are', 47246), ('have', 47011), ('be', 46709), ('not', 44061), ('but', 42634), ('they', 42499), ('at', 42245), ('do', 41723), ('what', 35786), ('from', 34702), ('his', 33609), ('by', 32861), ('or', 32280), ('all', 30252), ('she', 30008), ('my', 29416), ('an', 28691), ('about', 27869), ('so', 27507), ('there', 27373), ('one', 27128), ('her', 26401), ('had', 25676), ('if', 25430), ('me', 24875), ('your', 24687), ('who', 23555), ('can', 23406), ('out', 23357), ('their', 23236), ('no', 23179), ('has', 22791), ('up', 22668), ('were', 22508), ('like', 22124), ('when', 21978), ('just', 21765), ('would', 21669), ('more', 20965), ('will', 20664), ('m', 19310), ('know', 18882), ('said', 18678), ('re', 18667), ('did', 17627), ('been', 17483), ('people', 17398), ('time', 16973), ('get', 16662), ('how', 16157), ('them', 15922), ('some', 15646), ('now', 15004), ('which', 14937), ('him', 14681), ('could', 14372), ('think', 13889), ('than', 13849), ('into', 13654), ('our', 13610), ('other', 13596), ('well', 13524), ('right', 13411), ('here', 13047), ('new', 12863), ('because', 12566), ('over', 12302), ('then', 12223), ('see', 11946), ('go', 11721), ('only', 11543), ('back', 11509), ('these', 11452), ('two', 11391), ('going', 11352), ('first', 10909), ('its', 10732)]\n",
      "122598\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define the path to your text files\n",
    "directory = '../data/corpus/coca-samples-text'\n",
    "\n",
    "# Initialize a counter for the vocabulary\n",
    "vocabulary = Counter()\n",
    "\n",
    "# This regex matches only alphabetic sequences (i.e., words)\n",
    "word_pattern = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "# Read and process each file\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            # Normalize the text\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            # Find all valid words\n",
    "            words = word_pattern.findall(text)\n",
    "            \n",
    "            # Update the vocabulary counter with words\n",
    "            vocabulary.update(words)\n",
    "\n",
    "# Print the most common words\n",
    "print(vocabulary.most_common(100))  # Print the 100 most common words\n",
    "print(len(vocabulary))  # Print the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag '@5018041' is NOT present in the vocabulary.\n",
      "Tag '@5108241' is NOT present in the vocabulary.\n",
      "Tag '@5108341' is NOT present in the vocabulary.\n",
      "Tag '@5108141' is NOT present in the vocabulary.\n",
      "Tag '<p>' is NOT present in the vocabulary.\n",
      "Tag '!' is NOT present in the vocabulary.\n",
      "Tag 'p' is present in the vocabulary.\n",
      "Frequency of 'p': 108290\n",
      "Tag '5108141' is NOT present in the vocabulary.\n",
      "Tag 'test' is present in the vocabulary.\n",
      "Frequency of 'test': 1204\n",
      "Tag 'of' is present in the vocabulary.\n",
      "Frequency of 'of': 218668\n",
      "Tag 'again' is present in the vocabulary.\n",
      "Frequency of 'again': 4991\n",
      "Tag 'ok' is present in the vocabulary.\n",
      "Frequency of 'ok': 1476\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'vocabulary' is a Counter or set that contains your vocabulary\n",
    "\n",
    "# Define the tags you want to check\n",
    "tags = ['@5018041', '@5108241', '@5108341', '@5108141', '<p>', '!', 'p', '5108141', 'test', 'of', \"again\", \"ok\"]  # Example tags\n",
    "\n",
    "# Check if each tag is in the vocabulary\n",
    "for tag in tags:\n",
    "    if tag in vocabulary:\n",
    "        print(f\"Tag '{tag}' is present in the vocabulary.\")\n",
    "        print(f\"Frequency of '{tag}': {vocabulary[tag]}\")\n",
    "    else:\n",
    "        print(f\"Tag '{tag}' is NOT present in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary:\n",
    "    if word.startswith('@'):\n",
    "        print(word)  # Print the word starting with '@'\n",
    "    if not word.isalpha():\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 462883), ('to', 238874), ('and', 232584), ('of', 218668), ('a', 206816), ('in', 154602), ('i', 139623), ('that', 124059), ('you', 109927), ('it', 104072), ('is', 94201), ('for', 79002), ('on', 65490), ('was', 64461), ('with', 59800), ('he', 57779), ('this', 51981), ('as', 51304), ('we', 47814), ('are', 47246), ('have', 47011), ('be', 46709), ('not', 44061), ('but', 42634), ('they', 42499), ('at', 42245), ('do', 41723), ('what', 35786), ('from', 34702), ('his', 33609), ('by', 32861), ('or', 32280), ('all', 30252), ('she', 30008), ('my', 29416), ('an', 28691), ('about', 27869), ('so', 27507), ('there', 27373), ('one', 27128), ('her', 26401), ('had', 25676), ('if', 25430), ('me', 24875), ('your', 24687), ('who', 23555), ('can', 23406), ('out', 23357), ('their', 23236), ('no', 23179), ('has', 22791), ('up', 22668), ('were', 22508), ('like', 22124), ('when', 21978), ('just', 21765), ('would', 21669), ('more', 20965), ('will', 20664), ('know', 18882), ('said', 18678), ('did', 17627), ('been', 17483), ('people', 17398), ('time', 16973), ('get', 16662), ('how', 16157), ('them', 15922), ('some', 15646), ('now', 15004), ('which', 14937), ('him', 14681), ('could', 14372), ('think', 13889), ('than', 13849), ('into', 13654), ('our', 13610), ('other', 13596), ('well', 13524), ('right', 13411), ('here', 13047), ('new', 12863), ('because', 12566), ('over', 12302), ('then', 12223), ('see', 11946), ('go', 11721), ('only', 11543), ('back', 11509), ('these', 11452), ('two', 11391), ('going', 11352), ('first', 10909), ('its', 10732), ('even', 10698), ('also', 10660), ('good', 10633), ('way', 10511), ('after', 10307), ('us', 10258)]\n",
      "122022\n"
     ]
    }
   ],
   "source": [
    "# Valid single-letter words\n",
    "valid_single_letter_words = {'a', 'i'}\n",
    "valid_two_letter_words = {'am', 'an', 'as', 'at', 'ax', 'be', 'by', 'do', 'go', 'he', 'if', 'in', 'is', 'it', 'me', 'my', 'no', 'of', 'ok', 'on', 'or', 'ox', 'so', 'to', 'up', 'us', 'we'}\n",
    "\n",
    "# Function to filter vocabulary\n",
    "def filter_vocabulary(vocabulary):\n",
    "    filtered_vocab = Counter()\n",
    "    \n",
    "    for word, count in vocabulary.items():\n",
    "        if (len(word) == 1 and word not in valid_single_letter_words) or (len(word) == 2 and word not in valid_two_letter_words):\n",
    "            continue  # Skip this word\n",
    "        filtered_vocab[word] = count\n",
    "    \n",
    "    return filtered_vocab   \n",
    "\n",
    "# Filter the vocabulary\n",
    "filtered_vocabulary = filter_vocabulary(vocabulary)\n",
    "\n",
    "# Print the most common words after filtering\n",
    "print(filtered_vocabulary.most_common(100))\n",
    "print(len(filtered_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter vocabulary\n",
    "def filter_vocabulary_min_freq(vocabulary, min_count=2):\n",
    "    filtered_vocab = Counter()\n",
    "    \n",
    "    for word, count in vocabulary.items():\n",
    "        if count < min_count:\n",
    "            continue  # Skip this word\n",
    "        filtered_vocab[word] = count\n",
    "    \n",
    "    return filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the vocabulary\n",
    "filtered_vocabulary_len = filter_vocabulary_min_freq(filtered_vocabulary, min_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tcr', 20), ('cspi', 20), ('guenther', 20), ('barristers', 20), ('korgano', 20), ('angkatell', 20), ('eun', 20), ('niran', 20), ('iphones', 20), ('soulmate', 20), ('stephanopoulo', 20), ('nair', 20), ('vinita', 20), ('outfront', 20), ('kaine', 20), ('karr', 20), ('todays', 20), ('gaylin', 20), ('lac', 20), ('churkin', 20), ('ntsb', 20), ('brigitte', 20), ('ewell', 20), ('estrogen', 20), ('lainey', 20), ('authoraffiliation', 20), ('teesha', 20), ('solicitors', 20), ('dermot', 20), ('kosnik', 20), ('chtarri', 20), ('shatlow', 20), ('shoo', 20), ('guruji', 20), ('rambling', 20), ('threaded', 20), ('dissipation', 20), ('chromosome', 20), ('originalist', 20), ('farnsworth', 20), ('ptl', 20), ('kompetenz', 20), ('mammography', 20), ('hpv', 20), ('illustrators', 20), ('oncol', 20), ('leicester', 20), ('comaroff', 20), ('winked', 20), ('popov', 20), ('catchment', 20), ('cfd', 20), ('bargmann', 20), ('groundfish', 20), ('causative', 20), ('parallelism', 20), ('veronese', 20), ('emulation', 20), ('pollen', 20), ('emancipation', 20), ('growled', 20), ('bearings', 20), ('drumming', 20), ('bookcase', 20), ('dangerousness', 20), ('suckers', 20), ('sayles', 20), ('hassell', 20), ('intestinal', 20), ('celiac', 20), ('wyden', 20), ('fleas', 20), ('upa', 20), ('mort', 20), ('hens', 20), ('telegram', 20), ('hamm', 20), ('eso', 20), ('juveniles', 20), ('manson', 20), ('langner', 20), ('mcgarity', 20), ('jolie', 20), ('fordham', 20), ('charger', 20), ('sobel', 20), ('osu', 20), ('diehl', 20), ('mam', 20), ('pueblo', 20), ('jog', 20), ('instruct', 20), ('mariner', 20), ('illuminating', 20), ('savor', 20), ('bryce', 20), ('fullness', 20), ('dilemmas', 20), ('petra', 20), ('spca', 20)]\n",
      "19953\n"
     ]
    }
   ],
   "source": [
    "# Get the least common words by reversing the output of most_common()\n",
    "least_common_words = filtered_vocabulary_len.most_common()[::-1]\n",
    "\n",
    "# Print the least common words, e.g., the 100 least common words\n",
    "print(least_common_words[:100])\n",
    "print(len(least_common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122022\n",
      "19953\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_vocabulary))\n",
    "print(len(filtered_vocabulary_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we also need to add all the words from the phrase tests to be sure we will not have any problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/corpus/phraseWords.txt\", \"r\") as f:\n",
    "    phraseWords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in phraseWords:\n",
    "    if not word in filtered_vocabulary_len:\n",
    "        # Add it to the vocab with the frequency of min_count\n",
    "        filtered_vocabulary_len[word] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20015\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_vocabulary_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating a Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab = pd.DataFrame(filtered_vocabulary_len.items(), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iran</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nuclear</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>program</td>\n",
       "      <td>2389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>talks</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>photo</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  count\n",
       "0     iran    515\n",
       "1  nuclear    613\n",
       "2  program   2389\n",
       "3    talks    338\n",
       "4    photo   1387"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word   count  log_count\n",
      "0     iran     515   6.244167\n",
      "1  nuclear     613   6.418365\n",
      "2  program    2389   7.778630\n",
      "3    talks     338   5.823046\n",
      "4    photo    1387   7.234898\n",
      "5  updated     144   4.969813\n",
      "6      nov     302   5.710427\n",
      "7       is   94201  11.453186\n",
      "8      one   27128  10.208322\n",
      "9       of  218668  12.295310\n"
     ]
    }
   ],
   "source": [
    "# Add a column log_count to the DataFrame\n",
    "df_vocab['log_count'] = df_vocab['count'].apply(lambda x: np.log(x))\n",
    "\n",
    "# Show the top 10 frequencies and log frequencies\n",
    "print(df_vocab.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it as vocab_final.csv\n",
    "df_vocab.to_csv('../data/vocab_final.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glance-writer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
