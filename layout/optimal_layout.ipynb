{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finding the Optimal Layout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(keyboard_layout, corpus):\n",
    "    key_map = {}\n",
    "    for i, key in enumerate(keyboard_layout):\n",
    "        for letter in key:\n",
    "            key_map[letter] = i\n",
    "\n",
    "    word_patterns = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        pattern = []\n",
    "        last_key = None\n",
    "        for letter in word:\n",
    "            key = key_map[letter]\n",
    "            if key != last_key:\n",
    "                pattern.append(key)\n",
    "                last_key = key\n",
    "        word_patterns[tuple(pattern)] += 1\n",
    "\n",
    "    # The loss is the number of words that have the same pattern\n",
    "    loss = sum(count for count in word_patterns.values() if count > 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(current_layout, remaining_letters, max_keys, best_layout, best_loss, corpus):\n",
    "    # Check if we have assigned all letters and have at most max_keys keys\n",
    "    if not remaining_letters and len(current_layout) <= max_keys:\n",
    "        # Evaluate this layout\n",
    "        loss = compute_loss(current_layout, corpus)\n",
    "        if loss < best_loss[0]:\n",
    "            best_loss[0] = loss\n",
    "            best_layout[:] = current_layout[:]\n",
    "        return\n",
    "\n",
    "    # If already have max_keys and there are still letters left, return\n",
    "    if len(current_layout) >= max_keys:\n",
    "        return\n",
    "\n",
    "    # Try to group remaining letters into the next key, considering all possible splits\n",
    "    for i in range(1, len(remaining_letters) + 1):\n",
    "        new_key = remaining_letters[:i]\n",
    "        new_remaining = remaining_letters[i:]\n",
    "        backtrack(current_layout + [new_key], new_remaining, max_keys, best_layout, best_loss, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_keyboard(corpus, max_keys=12):\n",
    "    alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    best_layout = []\n",
    "    best_loss = [float('inf')]\n",
    "\n",
    "    backtrack([], alphabet, max_keys, best_layout, best_loss, corpus)\n",
    "\n",
    "    return best_layout, best_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Layout: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'LMNOPQRSTUVWXYZ']\n",
      "Minimal Loss: 0\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "corpus = [\"HELLO\", \"WORLD\", \"KEYBOARD\", \"CIRCUIT\", \"EXAMPLE\", \"PYTHON\", \"OPTIMIZATION\", \"TASK\"]\n",
    "optimal_layout, minimal_loss = find_optimal_keyboard(corpus)\n",
    "\n",
    "print(\"Optimal Layout:\", optimal_layout)\n",
    "print(\"Minimal Loss:\", minimal_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating Vocabulary**\n",
    "\n",
    "We are using the COCA samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 462883), ('to', 238874), ('and', 232584), ('of', 218668), ('a', 206816), ('in', 154602), ('i', 139623), ('that', 124059), ('you', 109927), ('p', 108290), ('s', 107201), ('it', 104072), ('is', 94201), ('for', 79002), ('on', 65490), ('was', 64461), ('with', 59800), ('he', 57779), ('this', 51981), ('t', 51527), ('as', 51304), ('n', 51142), ('we', 47814), ('are', 47246), ('have', 47011), ('be', 46709), ('not', 44061), ('but', 42634), ('they', 42499), ('at', 42245), ('do', 41723), ('what', 35786), ('from', 34702), ('his', 33609), ('by', 32861), ('or', 32280), ('all', 30252), ('she', 30008), ('my', 29416), ('an', 28691), ('about', 27869), ('so', 27507), ('there', 27373), ('one', 27128), ('her', 26401), ('had', 25676), ('if', 25430), ('me', 24875), ('your', 24687), ('who', 23555), ('can', 23406), ('out', 23357), ('their', 23236), ('no', 23179), ('has', 22791), ('up', 22668), ('were', 22508), ('like', 22124), ('when', 21978), ('just', 21765), ('would', 21669), ('more', 20965), ('will', 20664), ('m', 19310), ('know', 18882), ('said', 18678), ('re', 18667), ('did', 17627), ('been', 17483), ('people', 17398), ('time', 16973), ('get', 16662), ('how', 16157), ('them', 15922), ('some', 15646), ('now', 15004), ('which', 14937), ('him', 14681), ('could', 14372), ('think', 13889), ('than', 13849), ('into', 13654), ('our', 13610), ('other', 13596), ('well', 13524), ('right', 13411), ('here', 13047), ('new', 12863), ('because', 12566), ('over', 12302), ('then', 12223), ('see', 11946), ('go', 11721), ('only', 11543), ('back', 11509), ('these', 11452), ('two', 11391), ('going', 11352), ('first', 10909), ('its', 10732)]\n",
      "122598\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define the path to your text files\n",
    "directory = '../data/corpus/coca-samples-text'\n",
    "\n",
    "# Initialize a counter for the vocabulary\n",
    "vocabulary = Counter()\n",
    "\n",
    "# This regex matches only alphabetic sequences (i.e., words)\n",
    "word_pattern = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "# Read and process each file\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            # Normalize the text\n",
    "            text = text.lower()  # Convert to lowercase\n",
    "            # Find all valid words\n",
    "            words = word_pattern.findall(text)\n",
    "            \n",
    "            # Update the vocabulary counter with words\n",
    "            vocabulary.update(words)\n",
    "\n",
    "# Print the most common words\n",
    "print(vocabulary.most_common(100))  # Print the 100 most common words\n",
    "print(len(vocabulary))  # Print the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag '@5018041' is NOT present in the vocabulary.\n",
      "Tag '@5108241' is NOT present in the vocabulary.\n",
      "Tag '@5108341' is NOT present in the vocabulary.\n",
      "Tag '@5108141' is NOT present in the vocabulary.\n",
      "Tag '<p>' is NOT present in the vocabulary.\n",
      "Tag '!' is NOT present in the vocabulary.\n",
      "Tag 'p' is present in the vocabulary.\n",
      "Tag '5108141' is NOT present in the vocabulary.\n",
      "Tag 'test' is present in the vocabulary.\n",
      "Tag 'of' is present in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'vocabulary' is a Counter or set that contains your vocabulary\n",
    "\n",
    "# Define the tags you want to check\n",
    "tags = ['@5018041', '@5108241', '@5108341', '@5108141', '<p>', '!', 'p', '5108141', 'test', 'of']  # Example tags\n",
    "\n",
    "# Check if each tag is in the vocabulary\n",
    "for tag in tags:\n",
    "    if tag in vocabulary:\n",
    "        print(f\"Tag '{tag}' is present in the vocabulary.\")\n",
    "    else:\n",
    "        print(f\"Tag '{tag}' is NOT present in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary:\n",
    "    if word.startswith('@'):\n",
    "        print(word)  # Print the word starting with '@'\n",
    "    if not word.isalpha():\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 462883), ('to', 238874), ('and', 232584), ('a', 206816), ('in', 154602), ('i', 139623), ('that', 124059), ('you', 109927), ('it', 104072), ('is', 94201), ('for', 79002), ('on', 65490), ('was', 64461), ('with', 59800), ('he', 57779), ('this', 51981), ('as', 51304), ('we', 47814), ('are', 47246), ('have', 47011), ('be', 46709), ('not', 44061), ('but', 42634), ('they', 42499), ('at', 42245), ('do', 41723), ('what', 35786), ('from', 34702), ('his', 33609), ('by', 32861), ('or', 32280), ('all', 30252), ('she', 30008), ('my', 29416), ('an', 28691), ('about', 27869), ('so', 27507), ('there', 27373), ('one', 27128), ('her', 26401), ('had', 25676), ('if', 25430), ('me', 24875), ('your', 24687), ('who', 23555), ('can', 23406), ('out', 23357), ('their', 23236), ('no', 23179), ('has', 22791), ('up', 22668), ('were', 22508), ('like', 22124), ('when', 21978), ('just', 21765), ('would', 21669), ('more', 20965), ('will', 20664), ('know', 18882), ('said', 18678), ('did', 17627), ('been', 17483), ('people', 17398), ('time', 16973), ('get', 16662), ('how', 16157), ('them', 15922), ('some', 15646), ('now', 15004), ('which', 14937), ('him', 14681), ('could', 14372), ('think', 13889), ('than', 13849), ('into', 13654), ('our', 13610), ('other', 13596), ('well', 13524), ('right', 13411), ('here', 13047), ('new', 12863), ('because', 12566), ('over', 12302), ('then', 12223), ('see', 11946), ('go', 11721), ('only', 11543), ('back', 11509), ('these', 11452), ('two', 11391), ('going', 11352), ('first', 10909), ('its', 10732), ('even', 10698), ('also', 10660), ('good', 10633), ('way', 10511), ('after', 10307), ('us', 10258), ('where', 10067)]\n",
      "122020\n"
     ]
    }
   ],
   "source": [
    "# Valid single-letter words\n",
    "valid_single_letter_words = {'a', 'i'}\n",
    "valid_two_letter_words = {'am', 'an', 'as', 'at', 'ax', 'be', 'by', 'do', 'go', 'he', 'if', 'in', 'is', 'it', 'me', 'my', 'no', 'of,' 'ok', 'on', 'or', 'ox', 'so', 'to', 'up', 'us', 'we'}\n",
    "\n",
    "# Function to filter vocabulary\n",
    "def filter_vocabulary(vocabulary):\n",
    "    filtered_vocab = Counter()\n",
    "    \n",
    "    for word, count in vocabulary.items():\n",
    "        if (len(word) == 1 and word not in valid_single_letter_words) or (len(word) == 2 and word not in valid_two_letter_words):\n",
    "            continue  # Skip this word\n",
    "        filtered_vocab[word] = count\n",
    "    \n",
    "    return filtered_vocab   \n",
    "\n",
    "# Filter the vocabulary\n",
    "filtered_vocabulary = filter_vocabulary(vocabulary)\n",
    "\n",
    "# Print the most common words after filtering\n",
    "print(filtered_vocabulary.most_common(100))\n",
    "print(len(filtered_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter vocabulary\n",
    "def filter_vocabulary_min_freq(vocabulary, min_count=2):\n",
    "    filtered_vocab = Counter()\n",
    "    \n",
    "    for word, count in vocabulary.items():\n",
    "        if count < min_count:\n",
    "            continue  # Skip this word\n",
    "        filtered_vocab[word] = count\n",
    "    \n",
    "    return filtered_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the vocabulary\n",
    "filtered_vocabulary_len = filter_vocabulary_min_freq(filtered_vocabulary, min_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tcr', 20), ('cspi', 20), ('guenther', 20), ('barristers', 20), ('korgano', 20), ('angkatell', 20), ('eun', 20), ('niran', 20), ('iphones', 20), ('soulmate', 20), ('stephanopoulo', 20), ('nair', 20), ('vinita', 20), ('outfront', 20), ('kaine', 20), ('karr', 20), ('todays', 20), ('gaylin', 20), ('lac', 20), ('churkin', 20), ('ntsb', 20), ('brigitte', 20), ('ewell', 20), ('estrogen', 20), ('lainey', 20), ('authoraffiliation', 20), ('teesha', 20), ('solicitors', 20), ('dermot', 20), ('kosnik', 20), ('chtarri', 20), ('shatlow', 20), ('shoo', 20), ('guruji', 20), ('rambling', 20), ('threaded', 20), ('dissipation', 20), ('chromosome', 20), ('originalist', 20), ('farnsworth', 20), ('ptl', 20), ('kompetenz', 20), ('mammography', 20), ('hpv', 20), ('illustrators', 20), ('oncol', 20), ('leicester', 20), ('comaroff', 20), ('winked', 20), ('popov', 20), ('catchment', 20), ('cfd', 20), ('bargmann', 20), ('groundfish', 20), ('causative', 20), ('parallelism', 20), ('veronese', 20), ('emulation', 20), ('pollen', 20), ('emancipation', 20), ('growled', 20), ('bearings', 20), ('drumming', 20), ('bookcase', 20), ('dangerousness', 20), ('suckers', 20), ('sayles', 20), ('hassell', 20), ('intestinal', 20), ('celiac', 20), ('wyden', 20), ('fleas', 20), ('upa', 20), ('mort', 20), ('hens', 20), ('telegram', 20), ('hamm', 20), ('eso', 20), ('juveniles', 20), ('manson', 20), ('langner', 20), ('mcgarity', 20), ('jolie', 20), ('fordham', 20), ('charger', 20), ('sobel', 20), ('osu', 20), ('diehl', 20), ('mam', 20), ('pueblo', 20), ('jog', 20), ('instruct', 20), ('mariner', 20), ('illuminating', 20), ('savor', 20), ('bryce', 20), ('fullness', 20), ('dilemmas', 20), ('petra', 20), ('spca', 20)]\n",
      "19951\n"
     ]
    }
   ],
   "source": [
    "# Get the least common words by reversing the output of most_common()\n",
    "least_common_words = filtered_vocabulary_len.most_common()[::-1]\n",
    "\n",
    "# Print the least common words, e.g., the 100 least common words\n",
    "print(least_common_words[:100])\n",
    "print(len(least_common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122020\n",
      "19951\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_vocabulary))\n",
    "print(len(filtered_vocabulary_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD THE CORPUS PHRASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the vocabulary to a file\n",
    "with open('vocabulary.txt', 'w', encoding='utf-8') as vocab_file:\n",
    "    for word, count in vocabulary.items():\n",
    "        vocab_file.write(f'{word}: {count}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glance-writer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
