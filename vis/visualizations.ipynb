{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import glancewriterDecoder as gd\n",
    "import re\n",
    "sys.path.append(\"..\") #voodoo shit\n",
    "from trie.keyboard import create_keyboard\n",
    "from trie.trie import Node, insert_key\n",
    "from trie.predict import predict\n",
    "\n",
    "from clustering.TCluster import TCluster\n",
    "\n",
    "import os\n",
    "import json\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGazePathsFromFile(file_path):\n",
    "    try:\n",
    "        # Open and read the file\n",
    "        gazePaths = []\n",
    "        topWords = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.readlines()\n",
    "        for line in content:\n",
    "            # print(line)\n",
    "            if (line.strip().startswith('{')):\n",
    "                corrected_content = correct_json(line)\n",
    "                # print(corrected_content)\n",
    "                data = json.loads(corrected_content)\n",
    "                if \"top_words\" in data:\n",
    "                    topWords.append(data[\"top_words\"])\n",
    "                if \"gaze_points\" in data:\n",
    "                    points = data[\"gaze_points\"]\n",
    "                    points = [(point['x'], point['y'], point['z']) for point in points]\n",
    "                    gazePaths.append(points)\n",
    "        return gazePaths, topWords\n",
    "        # for i, path in enumerate(gazePaths):\n",
    "        #     print(i)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON format in the file: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def correct_json(content):\n",
    "    # Replace single quotes with double quotes\n",
    "    content = content.replace(\"'\", '\"')\n",
    "    \n",
    "    # Correct boolean values\n",
    "    content = content.replace('True', 'true').replace('False', 'false')\n",
    "    \n",
    "    # Remove trailing commas in lists and objects\n",
    "    content = re.sub(r',\\s*}', '}', content)\n",
    "    content = re.sub(r',\\s*]', ']', content)\n",
    "    \n",
    "    # Wrap the entire content in curly braces if it's not already\n",
    "    if not content.strip().startswith('{'):\n",
    "        content = '{' + content + '}'\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_paths, top_words = GetGazePathsFromFile(\"../eyeData/eyeTracking2024-10-02 15-48-10.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.34633731842041016, -0.04733343422412872), (-0.43738827109336853, -0.023766815662384033), (-0.39010536670684814, -0.03548897057771683), (-0.3468334674835205, -0.045335568487644196), (-0.30884596705436707, -0.04058998078107834), (-0.26352375745773315, -0.030061714351177216), (-0.27201828360557556, -0.0325019508600235), (-0.2778249979019165, -0.01756148785352707), (-0.2924300730228424, -0.03718181699514389), (-0.3109557330608368, -0.03505685180425644), (-0.2716694176197052, -0.037626639008522034), (-0.3100527822971344, -0.015218585729598999), (-0.2829872965812683, -0.03866095840930939), (-0.2997112274169922, -0.02582801878452301), (-0.3973274528980255, -0.020122364163398743), (-0.3909473717212677, 0.016609027981758118), (-0.3826747536659241, 0.019045390188694), (-0.3454795479774475, 0.00588805228471756), (-0.4309781789779663, 0.00205964595079422), (-0.45234251022338867, 0.03178364410996437), (-0.4536591172218323, 0.03692755103111267), (-0.4630325436592102, 0.030335139483213425), (-0.4729381203651428, 0.03862917050719261), (-0.4791968762874603, 0.028240419924259186), (-0.4332056939601898, 0.013710491359233856), (-0.3830491006374359, 0.001550436019897461), (-0.3987725079059601, 0.009556017816066742), (-0.4755018949508667, 0.01484459638595581), (-0.47050368785858154, 0.024224478751420975), (-0.46429526805877686, 0.015106335282325745), (-0.4577862024307251, -0.003636479377746582), (-0.4466792047023773, 0.011626757681369781), (-0.45455655455589294, 0.02039160206913948), (-0.2802984416484833, -0.058498889207839966), (0.021014541387557983, -0.07879678905010223), (0.2916926443576813, -0.16916020214557648), (0.3390635550022125, -0.1463586688041687), (0.3513328731060028, -0.18029898405075073), (0.3521803617477417, -0.17887255549430847), (0.4326178729534149, -0.1787521243095398), (0.3995895981788635, -0.1600237786769867), (0.41405370831489563, -0.13644243776798248), (0.40177908539772034, -0.16960538923740387), (0.40213742852211, -0.15640443563461304), (0.4238015115261078, -0.16053490340709686), (0.4193112850189209, -0.1740368902683258), (0.4420033097267151, -0.15168319642543793), (0.3778766095638275, -0.15151606500148773), (0.3370550870895386, -0.10878278315067291), (0.3441964387893677, -0.13390910625457764), (0.3486878275871277, -0.1479819267988205), (0.32729387283325195, -0.16429473459720612), (0.3249817490577698, -0.19826510548591614), (0.2754191756248474, -0.1952095925807953), (0.2550812065601349, -0.16484490036964417), (0.2439596951007843, -0.18853849172592163), (0.24299480020999908, -0.1645473837852478), (0.26161304116249084, -0.18955758213996887), (0.3004489243030548, -0.16931813955307007), (0.24832433462142944, -0.1912691593170166), (0.24619245529174805, -0.1885022521018982), (0.25420013070106506, -0.18974435329437256), (0.31027427315711975, -0.17275981605052948), (0.3602180778980255, -0.1524808555841446), (0.40578845143318176, -0.13443595170974731), (0.4065893888473511, -0.13238173723220825), (0.3677883446216583, -0.07967941462993622), (0.25484082102775574, 0.031246766448020935), (0.26492035388946533, -0.03728967905044556), (0.26919087767601013, 0.004076577723026276), (0.30344143509864807, 0.01199091225862503), (0.3272460401058197, 0.014391150325536728), (0.3379402458667755, 0.020837008953094482), (0.34088408946990967, 0.02364204451441765), (0.3740834593772888, 0.036137208342552185), (0.3677886724472046, 0.04009300842881203), (0.35158222913742065, 0.010123476386070251), (0.3121386766433716, 0.016531679779291153), (0.24582454562187195, 0.019703969359397888), (0.21451376378536224, 0.009769581258296967), (0.3163505494594574, 0.015047714114189148), (0.3611114025115967, 0.0001735389232635498), (0.411059707403183, 0.022165540605783463), (0.3996533155441284, 0.01694658026099205), (0.3351668119430542, 0.0030922070145606995), (0.3428773880004883, 0.009671740233898163), (0.3670829236507416, 0.0017291978001594543), (0.37166929244995117, -0.0003277882933616638), (0.3272722065448761, -0.004816778004169464), (0.30247214436531067, 0.018268045037984848), (0.2994336187839508, -0.016923345625400543), (0.32541853189468384, -0.011179856956005096), (0.359822541475296, 0.002956956624984741), (0.3491867184638977, -0.003812827169895172), (0.33117714524269104, 0.0016292780637741089), (0.32546231150627136, 0.0007144883275032043), (0.3232550024986267, 0.0031754299998283386), (0.3119240999221802, -0.009230002760887146), (0.3050956130027771, -0.009279541671276093), (0.2965809404850006, -0.008263535797595978), (0.3022593557834625, -0.0060890913009643555), (0.31289008259773254, -0.003899097442626953)]\n",
      "Top 5 candidates: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keyboard_layout = gd.read_keyboard_layout(\"circleLayout.txt\")\n",
    "decoder = gd.GlanceWriterDecoder(keyboard_layout)\n",
    "\n",
    "# Insert words into the trie\n",
    "# words = [\"move\", \"more\", \"mode\", \"mole\"]\n",
    "df_training = pd.read_csv('../data/vocab_final.csv')\n",
    "\n",
    "training_words = df_training['word'].tolist()\n",
    "\n",
    "# Filter only the words that are alpha\n",
    "training_words = [str(word).lower() for word in training_words if str(word).isalpha()]\n",
    "\n",
    "for word in training_words:\n",
    "    decoder.insert_word(word)\n",
    "\n",
    "# Simulate a gaze path (this should be replaced with actual gaze data)\n",
    "for i in range(1):\n",
    "    gaze_path = gaze_paths[i]\n",
    "    gaze_path = [(point[0], point[1]) for point in gaze_path]\n",
    "    print(gaze_path)\n",
    "    # with open(\"sample_path.txt\", \"r\") as file:\n",
    "    #     for line in file:\n",
    "    #         noParen = line[1:-2]\n",
    "    #         nums = noParen.split(\",\")\n",
    "    #         gaze_path.append((float(nums[0].strip()), float(nums[1].strip())))\n",
    "    # gaze_path = [(85, 55), (95, 45), (130, 15), (180, 35)]\n",
    "\n",
    "    # Decode the gaze path\n",
    "    result = decoder.decode_gaze_path(gaze_path)\n",
    "    print(\"Top 5 candidates:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.009399980306625366, -0.125)\n",
      "0.3230000138282776\n",
      "0.5\n",
      "offline predicted words: ['the', 'he', 'fee']\n",
      "words predicted live: ['the', 'he']\n",
      "offline predicted words: ['ledge', 'face', 'leaf']\n",
      "words predicted live: ['ledge', 'leaf', 'face']\n",
      "offline predicted words: ['stops', 'troops', 'crops']\n",
      "words predicted live: ['stops', 'troops', 'crops']\n",
      "offline predicted words: ['crops', 'drops', 'cops']\n",
      "words predicted live: ['crops', 'drops', 'cops']\n",
      "offline predicted words: ['too', 'up', 'to']\n",
      "words predicted live: ['too', 'up', 'to']\n",
      "offline predicted words: ['the', 'he', 'fee']\n",
      "words predicted live: ['the', 'he', 'fee']\n",
      "offline predicted words: ['ground', 'round', 'hound']\n",
      "words predicted live: ['ground', 'round', 'hound']\n",
      "offline predicted words: ['fee']\n",
      "words predicted live: []\n",
      "offline predicted words: ['the', 'he']\n",
      "words predicted live: ['the', 'he']\n",
      "offline predicted words: ['night', 'might', 'light']\n",
      "words predicted live: ['night', 'light', 'might']\n",
      "offline predicted words: ['giddy', 'sky', 'six']\n",
      "words predicted live: ['sky', 'sit', 'six']\n",
      "offline predicted words: ['is', 'i']\n",
      "words predicted live: ['is', 'i']\n",
      "offline predicted words: ['bright', 'slight', 'night']\n",
      "words predicted live: ['bright', 'slight', 'night']\n",
      "offline predicted words: ['stand', 'stood', 'sumo']\n",
      "words predicted live: ['sand', 'same', 'sync']\n",
      "offline predicted words: ['and', 'add', 'odd']\n",
      "words predicted live: ['and', 'add', 'an']\n",
      "offline predicted words: ['spy', 'spp']\n",
      "words predicted live: ['spy', 'spp']\n",
      "offline predicted words: ['clear', 'dear', 'cear']\n",
      "words predicted live: ['clear', 'dear', 'cear']\n",
      "offline predicted words: ['a']\n",
      "words predicted live: ['a']\n",
      "offline predicted words: ['small', 'mail', 'sail']\n",
      "words predicted live: ['small', 'mail', 'sail']\n",
      "offline predicted words: ['stream', 'steam', 'team']\n",
      "words predicted live: ['team', 'turf', 'tree']\n",
      "offline predicted words: ['stream', 'stefan', 'steam']\n",
      "words predicted live: ['stream', 'strode', 'trade']\n",
      "offline predicted words: ['helps', 'shows', 'flows']\n",
      "words predicted live: ['shells', 'shows', 'flows']\n",
      "offline predicted words: ['under', 'once', 'tone']\n",
      "words predicted live: ['under', 'deer', 'toe']\n",
      "offline predicted words: ['the', 'he']\n",
      "words predicted live: ['the', 'he']\n",
      "offline predicted words: ['bridge', 'ridge', 'bride']\n",
      "words predicted live: ['bridge', 'ridge', 'bride']\n",
      "offline predicted words: ['hostile', 'gentle', 'notice']\n",
      "words predicted live: ['hostile', 'gentle', 'these']\n",
      "offline predicted words: ['waves', 'wares', 'wards']\n",
      "words predicted live: ['waves', 'wares', 'wars']\n",
      "offline predicted words: ['lap', 'app', 'a']\n",
      "words predicted live: ['lap', 'app', 'a']\n",
      "offline predicted words: ['stat', 'sat', 'tat']\n",
      "words predicted live: ['stat', 'sat', 'tat']\n",
      "offline predicted words: ['at', 'a']\n",
      "words predicted live: ['at', 'a']\n",
      "offline predicted words: ['the', 'ate', 'aug']\n",
      "words predicted live: ['the', 'ate', 'aug']\n",
      "offline predicted words: ['shore', 'gore', 'hope']\n",
      "words predicted live: ['shore', 'gore', 'sore']\n",
      "offline predicted words: ['everyone', 'pursue', 'person']\n",
      "words predicted live: ['pursue', 'puerto', 'pattern']\n",
      "offline predicted words: ['everyone', 'froze', 'verne']\n",
      "words predicted live: ['everyone', 'extra', 'verne']\n",
      "offline predicted words: ['must', 'bust', 'but']\n",
      "words predicted live: ['must', 'bust', 'but']\n",
      "offline predicted words: ['know', 'now', 'ion']\n",
      "words predicted live: ['know', 'now', 'joy']\n",
      "offline predicted words: ['all', 'a']\n",
      "words predicted live: ['all', 'a']\n",
      "offline predicted words: ['the', 'he']\n",
      "words predicted live: ['the', 'he']\n",
      "offline predicted words: ['rules', 'files', 'tiles']\n",
      "words predicted live: ['rules', 'files', 'tiles']\n"
     ]
    }
   ],
   "source": [
    "vocab_path = os.path.join('../', 'data', 'vocab_final.csv')\n",
    "vocab = pd.read_csv(vocab_path)\n",
    "\n",
    "# Filter only the words that are alpha\n",
    "training_words = [str(word).lower() for word in training_words if str(word).isalpha()]\n",
    "root = Node()\n",
    "for word in training_words:\n",
    "    insert_key(root, word)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"circleLayout.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    corrected_text = correct_json(text)\n",
    "    layout = json.loads(corrected_text)\n",
    "    custom_keyboard = create_keyboard(layout[\"keyboard\"], useString=True)\n",
    "    center = (layout['center']['x'], layout['center']['y'])\n",
    "    inner_radius = layout['inner_radius']\n",
    "    outer_radius = layout['outer_radius']\n",
    "\n",
    "print(center)\n",
    "print(inner_radius)\n",
    "print(outer_radius)\n",
    "bigram_path = os.path.join('..', 'data', 'bigram_v2.json')\n",
    "truth = [\"the\", \"leaf\", \"drops\", \"drops\", \"to\", \"the\", \"ground\", \"\", \"the\", \"night\", \"sky\", \"is\",\n",
    " \"bright\", \"and\", \"and\", \"clear\", \"clear\", \"a\", \"small\", \"stream\", \"stream\", \"flows\", \"under\", \"the\", \"bridge\",\n",
    "  \"gentle\", \"waves\", \"lap\", \"at\", \"at\", \"the\", \"shore\", \"everyone\", \"everyone\", \"must\", \"know\", \"all\", \"the\", \"rules\", \"\", \"\", \"\"]\n",
    "with open(bigram_path, 'r') as f:\n",
    "    bigram_probs: dict[dict] = json.load(f)\n",
    "\n",
    "\n",
    "for i in range(len(top_words)):\n",
    "    # gaze_path = gaze_paths[i]\n",
    "    last_two: list[str] = ['<s>', '<s>']\n",
    "    context_probs: dict[dict] = bigram_probs.get(' '.join(last_two), {})\n",
    "    tc = TCluster(K=1, eps=0.07, vocab=vocab, context_probs=context_probs)\n",
    "\n",
    "    filtered_gaze_path = gaze_paths[i]\n",
    "    filtered_gaze_path = [(point[0], point[1], point[2]) for point in filtered_gaze_path if ((point[0] - center[0])**2 + (point[1] - center[1])**2 > inner_radius**2 and \n",
    "                                                                        (point[0] - center[0])**2 + (point[1] - center[1])**2 < outer_radius**2)]\n",
    "    df = pd.DataFrame(filtered_gaze_path, columns=['x', 'y', 'time'])\n",
    "    \n",
    "    tc.fit(df)\n",
    "    \n",
    "    keys = tc.predict(custom_keyboard, root)\n",
    "    predictions = [key[0] for key in keys]\n",
    "    if (truth[i] in predictions):\n",
    "        score += 1\n",
    "    # print(truth[i])\n",
    "    # print(str(predictions))\n",
    "    print(\"offline predicted words: \" + str(predictions))\n",
    "    print(\"words predicted live: \" + str(top_words[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    score = 0\n",
    "    eep = float(k+1)/100.0\n",
    "    for i in range(len(top_words)):\n",
    "        # gaze_path = gaze_paths[i]\n",
    "        last_two: list[str] = ['<s>', '<s>']\n",
    "        context_probs: dict[dict] = bigram_probs.get(' '.join(last_two), {})\n",
    "        tc = TCluster(K=1, eps=eep, vocab=vocab, context_probs=context_probs)\n",
    "\n",
    "        filtered_gaze_path = gaze_paths[i]\n",
    "        filtered_gaze_path = [(point[0], point[1], point[2]) for point in filtered_gaze_path if ((point[0] - center[0])**2 + (point[1] - center[1])**2 > inner_radius**2 and \n",
    "                                                                            (point[0] - center[0])**2 + (point[1] - center[1])**2 < outer_radius**2)]\n",
    "        df = pd.DataFrame(filtered_gaze_path, columns=['x', 'y', 'time'])\n",
    "        \n",
    "        tc.fit(df)\n",
    "        \n",
    "        keys = tc.predict(custom_keyboard, root)\n",
    "        predictions = [key[0] for key in keys]\n",
    "        if (truth[i] in predictions):\n",
    "            score += 1\n",
    "        # print(truth[i])\n",
    "        # print(str(predictions))\n",
    "        # print(\"offline predicted words: \" + str(predictions))\n",
    "        # print(\"words predicted live: \" + str(top_words[i]))\n",
    "    print(score)\n",
    "    print(eep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
